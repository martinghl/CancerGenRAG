{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c08adc6-17a5-47a7-9e4c-8b7df5284b59",
   "metadata": {},
   "source": [
    "# PGS data processing\n",
    "\n",
    "This script is specifically designed to automate the process of downloading Alzheimer's Disease (AD) research data from the PGS catalog. It focuses on extracting files from FTP links provided in a CSV file and ensures that downloaded data undergoes preliminary data cleaning and quality control.\n",
    "\n",
    "Script Purpose\n",
    "*  Targeted Downloading: Automates the downloading of AD research data files from the PGS catalog, addressing the needs of researchers focusing on genetic contributions to Alzheimer's Disease.\n",
    "* Data Cleaning and Quality Control: While the script prepares data for cleaning and quality checks, these processes are conducted separately to ensure data integrity and usability.\n",
    "s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc54043-4383-4c0a-9ff7-63ad59065942",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6918decf-e57f-401f-b323-727e75609002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0ae7e-8f33-4b8f-92ea-1d6331d69740",
   "metadata": {},
   "source": [
    "## Data Download And Decompress\n",
    "1. **Reads the CSV**: Loads the CSV file and extracts file URLs from the specified column.\n",
    "2. **Downloads Files**: Downloads each file from its URL to the specified folder.\n",
    "3. **Checks Existing Files**: Prevents re-downloading files that already exist.\n",
    "4. **Logs Status**: Outputs the status of each file download, noting both successes and failures.\n",
    "5. **Decompress**: Outputs the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9b1752-2f9f-4aa4-8302-fff1792a9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the CSV file and the download directory\n",
    "csv_file_path = os.path.join(r\"\", \"pgs_scores_lungcancer.csv\")\n",
    "download_folder = os.path.join(r\"\", \"CancerPGS\")\n",
    "\n",
    "# Ensure the download directory exists\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "# Specify the directory where decompressed files should be stored\n",
    "decompression_folder = os.path.join(download_folder, \"PGS_Decompress\")\n",
    "\n",
    "# Ensure the decompression directory exists\n",
    "os.makedirs(decompression_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba73121-c52d-479a-aa33-03238a4d28dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and decompressing files:   0%|                                                      | 0/34 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnew_filename\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already exists.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDownload complete!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'new_filename' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv(csv_file_path)\n",
    "compressed_files = []\n",
    "# Process each row to handle file downloads and renaming\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Downloading and decompressing files\"):\n",
    "    file_url = row['Scoring File (FTP Link)']  # Extract the FTP link\n",
    "    original_filename = file_url.split('/')[-1]  # Get the original filename from the URL\n",
    "    \n",
    "    core_filename = original_filename.split('.')[0]  # Extract the core identifier before the first '.'\n",
    "\n",
    "    file_url = file_url.replace(original_filename, 'Harmonized/'+core_filename +'_hmPOS_GRCh38.txt.gz')\n",
    "    file_path = os.path.join(download_folder, core_filename +'_hmPOS_GRCh38.txt.gz')  # Update the path with the new filename\n",
    "    compressed_files.append(core_filename +'_hmPOS_GRCh38.txt.gz')\n",
    "    # Check if the new file already exists\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            print(f\"Downloading and renaming {original_filename} to {core_filename}...\")\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Failed to download {original_filename}. Status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading {original_filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"{new_filename} already exists.\")\n",
    "print(f'Download complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c7191d-2a27-4595-a94c-f93813982967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:   3%|█▊                                                            | 1/34 [00:00<00:05,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Decompressed PGS000388_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000388_hmPOS_GRCh38.txt\n",
      "2\n",
      "Decompressed PGS000391_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000391_hmPOS_GRCh38.txt\n",
      "3\n",
      "Decompressed PGS000389_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000389_hmPOS_GRCh38.txt\n",
      "4\n",
      "Decompressed PGS000390_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000390_hmPOS_GRCh38.txt\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:  24%|██████████████▌                                               | 8/34 [00:00<00:01, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS000393_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000393_hmPOS_GRCh38.txt\n",
      "6\n",
      "Decompressed PGS000721_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000721_hmPOS_GRCh38.txt\n",
      "7\n",
      "Decompressed PGS000392_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000392_hmPOS_GRCh38.txt\n",
      "8\n",
      "Decompressed PGS000394_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000394_hmPOS_GRCh38.txt\n",
      "9\n",
      "Decompressed PGS003393_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS003393_hmPOS_GRCh38.txt\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:  29%|█████████████████▉                                           | 10/34 [00:00<00:01, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS000395_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000395_hmPOS_GRCh38.txt\n",
      "11\n",
      "Decompressed PGS003392_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS003392_hmPOS_GRCh38.txt\n",
      "12\n",
      "Decompressed PGS005169_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS005169_hmPOS_GRCh38.txt\n",
      "13\n",
      "Decompressed PGS004164_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004164_hmPOS_GRCh38.txt\n",
      "14\n",
      "Decompressed PGS003391_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS003391_hmPOS_GRCh38.txt\n",
      "15\n",
      "Decompressed PGS000396_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000396_hmPOS_GRCh38.txt\n",
      "16\n",
      "Decompressed PGS004246_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004246_hmPOS_GRCh38.txt\n",
      "17\n",
      "Decompressed PGS004165_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004165_hmPOS_GRCh38.txt\n",
      "18\n",
      "Decompressed PGS000397_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000397_hmPOS_GRCh38.txt\n",
      "19\n",
      "Decompressed PGS004325_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004325_hmPOS_GRCh38.txt\n",
      "20\n",
      "Decompressed PGS000789_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000789_hmPOS_GRCh38.txt\n",
      "21\n",
      "Decompressed PGS002808_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS002808_hmPOS_GRCh38.txt\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:  65%|███████████████████████████████████████▍                     | 22/34 [00:00<00:00, 29.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS004442_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004442_hmPOS_GRCh38.txt\n",
      "23\n",
      "Decompressed PGS005163_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS005163_hmPOS_GRCh38.txt\n",
      "24\n",
      "Decompressed PGS002270_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS002270_hmPOS_GRCh38.txt\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:  74%|████████████████████████████████████████████▊                | 25/34 [00:01<00:00, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS004860_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004860_hmPOS_GRCh38.txt\n",
      "26\n",
      "Decompressed PGS000078_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000078_hmPOS_GRCh38.txt\n",
      "27\n",
      "Decompressed PGS004955_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004955_hmPOS_GRCh38.txt\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files:  94%|█████████████████████████████████████████████████████████▍   | 32/34 [00:01<00:00, 18.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS004691_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004691_hmPOS_GRCh38.txt\n",
      "29\n",
      "Decompressed PGS000740_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000740_hmPOS_GRCh38.txt\n",
      "30\n",
      "Decompressed PGS000880_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000880_hmPOS_GRCh38.txt\n",
      "31\n",
      "Decompressed PGS000070_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000070_hmPOS_GRCh38.txt\n",
      "32\n",
      "Decompressed PGS004884_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004884_hmPOS_GRCh38.txt\n",
      "33\n",
      "Decompressed PGS000156_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS000156_hmPOS_GRCh38.txt\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files: 100%|█████████████████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed PGS004512_hmPOS_GRCh38.txt.gz to CancerPGS/PGS_Decompress/PGS004512_hmPOS_GRCh38.txt\n",
      "All files have been decompressed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all .gz files in the download directory\n",
    "gz_files = [f for f in os.listdir(download_folder) if f.endswith('.gz')]\n",
    "i = 0\n",
    "# Iterate over the list of files and decompress each one\n",
    "for gz_file in tqdm(gz_files, desc='Decompressing files'):\n",
    "    i += 1\n",
    "    print(i)\n",
    "    file_path = os.path.join(download_folder, gz_file)\n",
    "    decompressed_file_path = os.path.join(decompression_folder, gz_file[:-3])  # Removes '.gz'\n",
    "\n",
    "    # Open the gzipped file and decompress it\n",
    "    with gzip.open(file_path, 'rb') as gzipped_file:\n",
    "        with open(decompressed_file_path, 'wb') as decompressed_file:\n",
    "            decompressed_file.write(gzipped_file.read())\n",
    "            print(f\"Decompressed {gz_file} to {decompressed_file_path}\")\n",
    "\n",
    "print(\"All files have been decompressed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66b1a2-5dc1-4fc3-a778-ea29505797ef",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdd09d-da97-49d9-93d5-9b9155ba35c2",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "1. **Annotation**: we run the pipeline to annotate the files and fill the missing data with NA\n",
    "2. **Removing the unknown rsID**: remove the unknown rsID\n",
    "3. **File selection**: exclude the file raise errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ce893-c74e-4c2a-ad01-32a21fdee8cf",
   "metadata": {},
   "source": [
    "### For Files with issues, we annoate the hm_rsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e052121b-f703-4c66-ad53-b7e9969484d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_filenames = ['PGS003957','PGS003958']\n",
    "exclude_filenames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b576d2-6c74-442f-b57f-9a2ee2bce554",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "1. **scan all files and get a annotation map**: the annotation map has columns of ['SNP_coord', 'hm_rsID', 'hm_chr', 'hm_pos', 'effect_allele'] with no duplicate variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fe0189-3b63-4153-920d-27cd20730e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "            SNP_coord      hm_rsID hm_chr       hm_pos effect_allele\n",
      "0          4_12348664   rs73225917      4   12348664.0             A\n",
      "1        11_119542229          NaN     11  119542229.0             T\n",
      "2          4_46680688  rs138116934      4   46680688.0             T\n",
      "3         17_65392050          NaN     17   65392050.0             C\n",
      "4          1_17117119  rs116308950      1   17117119.0             A\n",
      "...               ...          ...    ...          ...           ...\n",
      "9365873    10_2169843    rs1984070     10    2169843.0             A\n",
      "9365881   1_217334283   rs12124075      1  217334283.0             T\n",
      "9365883    6_32789520    rs9276711      6   32789520.0             T\n",
      "9365894    6_29970056    rs2256543      6   29970056.0             T\n",
      "9365895    6_33314404    rs3130099      6   33314404.0             G\n",
      "\n",
      "[3053308 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing your CSV files\n",
    "directory = r'/Users/martinli/Desktop/CancerGeneBot/Notebook/CancerPGS/AnnotatedClean_ReAnnoted'\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "i = 0\n",
    "# Loop through all the CSV files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    i += 1\n",
    "    print(i)\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath, low_memory=False)  # Read the CSV file\n",
    "        dataframes.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "full_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Select unique rows based on specific columns\n",
    "annotation_map = full_df[['SNP_coord', 'hm_rsID', 'hm_chr', 'hm_pos', 'effect_allele']].drop_duplicates()\n",
    "\n",
    "# Optionally, save the unique DataFrame to a new CSV file\n",
    "annotation_map.to_csv(r'/Users/martinli/Desktop/CancerGeneBot/data/annotation_map.csv', index=False)\n",
    "\n",
    "# Print the unique DataFrame\n",
    "print(annotation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007092d-8807-43fe-9bcd-b537ac2a8ab0",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "1. **Detect duplicates**: Duplicate CSV files are defined as the files containing the same set of rsID\n",
    "2. **Merge duplicates**: Convert the effect weight to z-score and merge the duplicates by sum, append the ranks into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdeceda5-6105-49ff-b8d2-a1a46f5837f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Scanning 31 CSVs in: /Users/martinli/Desktop/CancerGeneBot/Notebook/CancerPGS/AnnotatedClean_ReAnnoted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:80: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:80: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:80: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collected 31 candidate studies for duplicate check.\n",
      "  Found 5 duplicate groups.\n",
      "    * Group PGS000389: ['PGS000389', 'PGS000390']\n",
      "    * Group PGS004325: ['PGS000721', 'PGS004325']\n",
      "    * Group PGS004512: ['PGS004442', 'PGS004512']\n",
      "    * Group PGS000789: ['PGS000078', 'PGS000789']\n",
      "    * Group PGS000393: ['PGS000393', 'PGS000394']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:129: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:129: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/yv/682pv_nx7q561mgzzkj5sr280000gn/T/ipykernel_78865/923256879.py:129: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] Built z-scores for 31 studies.\n",
      "[Step 3] After merging, 26 study entries remain.\n",
      "  example PGS000397: (46919, 3), cols=['hm_rsID', 'effect_weight', 'ranks']\n",
      "[Step 4] chosen_N=287  (score=1011, max=1122)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 327\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Step 4] chosen_N=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchosen_N\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (score=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchosen_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    326\u001b[39m \u001b[38;5;66;03m# E) standardize and rerank\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m processed_data = \u001b[43mstandardize_effect_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_N\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchosen_N\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Step 5] Standardized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m studies.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# F) save\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36mstandardize_effect_weights\u001b[39m\u001b[34m(merged, chosen_N, effect_size_col, rank_col)\u001b[39m\n\u001b[32m    268\u001b[39m tmp[effect_size_col] = tmp[effect_size_col] / np.sqrt(tmp[\u001b[33m'\u001b[39m\u001b[33mn_ranks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# collapse ranks to their mean\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrank_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# if any NaN avg ranks, push them to the bottom\u001b[39;00m\n\u001b[32m    273\u001b[39m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m] = tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].fillna(tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].max() + \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].isna().all() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1e9\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36mstandardize_effect_weights.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    268\u001b[39m tmp[effect_size_col] = tmp[effect_size_col] / np.sqrt(tmp[\u001b[33m'\u001b[39m\u001b[33mn_ranks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# collapse ranks to their mean\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m] = tmp[rank_col].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01melse\u001b[39;00m np.nan)\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# if any NaN avg ranks, push them to the bottom\u001b[39;00m\n\u001b[32m    273\u001b[39m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m] = tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].fillna(tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].max() + \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tmp[\u001b[33m'\u001b[39m\u001b[33mavg_rank\u001b[39m\u001b[33m'\u001b[39m].isna().all() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1e9\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   3857\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3860\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3861\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/numpy/_core/_methods.py:122\u001b[39m, in \u001b[36m_mean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m    118\u001b[39m arr = asanyarray(a)\n\u001b[32m    120\u001b[39m is_float16_result = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m rcount = \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rcount == \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount == \u001b[32m0\u001b[39m, axis=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    124\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mMean of empty slice.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Bio/lib/python3.13/site-packages/numpy/_core/_methods.py:80\u001b[39m, in \u001b[36m_count_reduce_items\u001b[39m\u001b[34m(arr, axis, keepdims, where)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# no boolean mask given, calculate items according to axis\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         axis = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     82\u001b[39m         axis = (axis,)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### CLEAN STEP 3\n",
    "\n",
    "import os, re, pickle, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "ANNOTATED_DIR = r\"/Users/martinli/Desktop/CancerGeneBot/Notebook/CancerPGS/AnnotatedClean_ReAnnoted\"\n",
    "OUT_PKL      = r\"/Users/martinli/Desktop/CancerGeneBot/Notebook/CancerPGS/data_dict.pkl\"\n",
    "MAX_N_SWEEP  = 500\n",
    "FRACTION     = 0.90   # target fraction of max overlap for choosing N\n",
    "\n",
    "# --------- UTILITIES ---------\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "    def find(self, x):\n",
    "        # robust find (auto-initialize)\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            return x\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    def union(self, x, y):\n",
    "        rx, ry = self.find(x), self.find(y)\n",
    "        if rx != ry:\n",
    "            self.parent[ry] = rx\n",
    "\n",
    "def extract_file_id(filename:str):\n",
    "    m = re.match(r\"(PGS\\d+)_\", filename)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _as_nonnull_str_set(series: pd.Series) -> set:\n",
    "    # dropna and cast to str to avoid NaN set-equality weirdness\n",
    "    return set(series.dropna().astype(str).tolist())\n",
    "\n",
    "def _build_SNP_coord_if_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'SNP_coord' in df.columns:\n",
    "        return df\n",
    "    # Try common chr/pos pairs\n",
    "    candidates = [\n",
    "        ('hm_chr','hm_pos'),\n",
    "        ('chrom','pos'),\n",
    "        ('chr','pos')\n",
    "    ]\n",
    "    for c, p in candidates:\n",
    "        if c in df.columns and p in df.columns:\n",
    "            # keep integer when possible; fall back to string\n",
    "            pos = pd.to_numeric(df[p], errors='coerce').astype('Int64')\n",
    "            df = df.copy()\n",
    "            df['SNP_coord'] = df[c].astype(str).str.strip() + \":\" + pos.astype(str)\n",
    "            return df\n",
    "    # If we cannot build it, just create empty placeholder\n",
    "    df = df.copy()\n",
    "    df['SNP_coord'] = pd.Series([pd.NA] * len(df))\n",
    "    return df\n",
    "\n",
    "def find_duplicate_files(folder_path: str):\n",
    "    \"\"\"\n",
    "    Flags studies that are duplicates by exact set equality on SNP_coord or hm_rsID.\n",
    "    Returns: (groups, file_data)\n",
    "      - groups: dict[root_id] = [member_ids...]\n",
    "      - file_data: dict[file_id] = raw DataFrame (with SNP_coord ensured)\n",
    "    \"\"\"\n",
    "    uf = UnionFind()\n",
    "    file_sets = {}\n",
    "    file_data = {}\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    print(f\"[Step 1] Scanning {len(files)} CSVs in: {folder_path}\")\n",
    "\n",
    "    for fn in files:\n",
    "        fid = extract_file_id(fn)\n",
    "        if not fid:\n",
    "            continue\n",
    "        path = os.path.join(folder_path, fn)\n",
    "        df = pd.read_csv(path)\n",
    "        df = _build_SNP_coord_if_missing(df)\n",
    "\n",
    "        # seed UF\n",
    "        uf.find(fid)\n",
    "\n",
    "        if ('SNP_coord' in df.columns) and ('hm_rsID' in df.columns):\n",
    "            snp_set = _as_nonnull_str_set(df['SNP_coord'])\n",
    "            rsid_set = _as_nonnull_str_set(df['hm_rsID'])\n",
    "            file_sets[fid] = {'SNP_coord': snp_set, 'hm_rsID': rsid_set}\n",
    "            file_data[fid] = df\n",
    "        else:\n",
    "            print(f\"  - Skipping {fn}: missing required columns.\")\n",
    "\n",
    "    ids = list(file_sets.keys())\n",
    "    print(f\"  Collected {len(ids)} candidate studies for duplicate check.\")\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        for j in range(i+1, len(ids)):\n",
    "            a, b = ids[i], ids[j]\n",
    "            if (file_sets[a]['SNP_coord'] == file_sets[b]['SNP_coord']) or \\\n",
    "               (file_sets[a]['hm_rsID']   == file_sets[b]['hm_rsID']):\n",
    "                uf.union(a, b)\n",
    "\n",
    "    # Build groups\n",
    "    groups = {}\n",
    "    for fid in ids:\n",
    "        r = uf.find(fid)\n",
    "        groups.setdefault(r, []).append(fid)\n",
    "\n",
    "    # Keep only groups with >1 (true dup groups). Singletons aren’t “duplicates”.\n",
    "    dup_groups = {root: members for root, members in groups.items() if len(members) > 1}\n",
    "    print(f\"  Found {len(dup_groups)} duplicate groups.\")\n",
    "    for gid, members in dup_groups.items():\n",
    "        print(f\"    * Group {gid}: {sorted(members)}\")\n",
    "\n",
    "    return dup_groups, file_data\n",
    "\n",
    "def convert_to_z_scores(\n",
    "    file_path: str,\n",
    "    effect_size_col: str = 'effect_weight',\n",
    "    rank_col: str = 'ranks',\n",
    "    effect_allele_col: str = 'effect_allele',\n",
    "    multiply_by_effect_allele_count: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns df with ['hm_rsID','effect_weight','ranks'] where effect_weight are Z-scores of |effect_size_col|.\n",
    "    If rank_col is missing, create rank from descending |effect|.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Required columns\n",
    "    if 'hm_rsID' not in df.columns:\n",
    "        raise ValueError(f\"{os.path.basename(file_path)} missing 'hm_rsID'.\")\n",
    "\n",
    "    # numeric effects\n",
    "    if effect_size_col not in df.columns:\n",
    "        raise ValueError(f\"{os.path.basename(file_path)} missing '{effect_size_col}'.\")\n",
    "    eff = pd.to_numeric(df[effect_size_col], errors='coerce').abs()\n",
    "\n",
    "    if multiply_by_effect_allele_count:\n",
    "        if effect_allele_col not in df.columns:\n",
    "            raise ValueError(f\"{os.path.basename(file_path)} missing '{effect_allele_col}'.\")\n",
    "        def allele_count(s):\n",
    "            if pd.isna(s): return 0\n",
    "            # count letters only (e.g., \"A/G\" -> 2)\n",
    "            return len(re.findall(r'[A-Za-z]', str(s)))\n",
    "        counts = df[effect_allele_col].apply(allele_count)\n",
    "        eff = eff * counts\n",
    "\n",
    "    mu = eff.mean(skipna=True)\n",
    "    sd = eff.std(skipna=True)\n",
    "    z = (eff - mu) / sd if (sd is not None and sd != 0 and not pd.isna(sd)) else pd.Series(0.0, index=eff.index)\n",
    "\n",
    "    # Prepare ranks\n",
    "    if rank_col in df.columns:\n",
    "        ranks = pd.to_numeric(df[rank_col], errors='coerce')\n",
    "        # fill if missing\n",
    "        if ranks.isna().any():\n",
    "            # fallback rank from z (descending)\n",
    "            ranks = (-z).rank(method='first')\n",
    "    else:\n",
    "        # build rank from z (descending)\n",
    "        ranks = (-z).rank(method='first')\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'hm_rsID': df['hm_rsID'].astype(str),\n",
    "        'effect_weight': z.astype(float),\n",
    "        'ranks': ranks.astype(float)\n",
    "    }).dropna(subset=['hm_rsID'])\n",
    "    return out\n",
    "\n",
    "def merge_duplicate_dataframes(data_dict: Dict[str,pd.DataFrame],\n",
    "                               dup_groups: Dict[str,List[str]],\n",
    "                               effect_size_col='effect_weight',\n",
    "                               rank_col='ranks') -> Dict[str,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each duplicate group, concatenate and aggregate by hm_rsID:\n",
    "      - sum effect_weight\n",
    "      - collect ranks as list\n",
    "    \"\"\"\n",
    "    merged = data_dict.copy()\n",
    "    to_remove = set()\n",
    "\n",
    "    for root, members in dup_groups.items():\n",
    "        dfs = []\n",
    "        for mid in members:\n",
    "            if mid in merged:\n",
    "                dfs.append(merged[mid])\n",
    "                if mid != root:\n",
    "                    to_remove.add(mid)\n",
    "            else:\n",
    "                print(f\"  ! Warning: {mid} missing from data_dict; skipping\")\n",
    "        if dfs:\n",
    "            combined = pd.concat(dfs, ignore_index=True)\n",
    "            agg = combined.groupby('hm_rsID', as_index=False).agg(\n",
    "                **{\n",
    "                    effect_size_col: (effect_size_col,'sum'),\n",
    "                    rank_col: (rank_col, lambda x: list(x))\n",
    "                }\n",
    "            )\n",
    "            merged[root] = agg\n",
    "\n",
    "    for mid in to_remove:\n",
    "        merged.pop(mid, None)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def filter_top_N_advanced(df, N=200, weight_column='effect_weight'):\n",
    "    if len(df) > N:\n",
    "        q = 1 - (N / len(df))\n",
    "        thr = df[weight_column].quantile(q)\n",
    "        keep = df[df[weight_column] >= thr]\n",
    "        return keep.nlargest(N, weight_column) if len(keep) > N else keep\n",
    "    return df\n",
    "\n",
    "def overlap_metric(variant_sets: List[set]) -> int:\n",
    "    \"\"\"\n",
    "    Sum over variants of C(count,2).\n",
    "    \"\"\"\n",
    "    all_vars = [v for s in variant_sets for v in s]\n",
    "    cnt = Counter(all_vars)\n",
    "    return sum((c*(c-1))//2 for c in cnt.values())\n",
    "\n",
    "def maximize_overlap_fraction_threshold(studies: List[pd.DataFrame],\n",
    "                                        id_column='hm_rsID',\n",
    "                                        weight_column='effect_weight',\n",
    "                                        max_N=500,\n",
    "                                        fraction=0.9) -> Tuple[int,int,int]:\n",
    "    scores = []\n",
    "    for N in range(1, max_N+1):\n",
    "        filtered = [filter_top_N_advanced(df, N=N, weight_column=weight_column) for df in studies]\n",
    "        vsets = [set(s[id_column].dropna().astype(str)) for s in filtered if id_column in s.columns and not s.empty]\n",
    "        sc = overlap_metric(vsets) if vsets else 0\n",
    "        scores.append((N, sc))\n",
    "    max_score = max(x for _, x in scores) if scores else 0\n",
    "    thresh = fraction * max_score\n",
    "    # smallest N with score >= thresh\n",
    "    chosen = next(((n, s) for (n, s) in scores if s >= thresh), (1, 0))\n",
    "    return chosen[0], chosen[1], max_score\n",
    "\n",
    "def standardize_effect_weights(merged: Dict[str,pd.DataFrame],\n",
    "                               chosen_N: int,\n",
    "                               effect_size_col='effect_weight',\n",
    "                               rank_col='ranks') -> Dict[str,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each study:\n",
    "      - compute n_ranks = len(rank list) BEFORE collapsing to mean\n",
    "      - standardize effect by dividing by sqrt(n_ranks)\n",
    "      - collapse ranks to mean\n",
    "      - rerank ascending by mean rank (1..k)\n",
    "      - then keep top N by effect if > N\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for fid, df in merged.items():\n",
    "        tmp = df.copy()\n",
    "\n",
    "        # ensure list in ranks\n",
    "        # (after merge, ranks is list; if singletons slipped in, wrap them)\n",
    "        def _ensure_list(x):\n",
    "            if isinstance(x, list):\n",
    "                return x\n",
    "            return [x] if not pd.isna(x) else []\n",
    "\n",
    "        tmp[rank_col] = tmp[rank_col].apply(_ensure_list)\n",
    "        tmp['n_ranks'] = tmp[rank_col].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        tmp['n_ranks'] = tmp['n_ranks'].replace(0, 1)  # avoid div/0 for rare oddities\n",
    "\n",
    "        # standardize by sqrt(n)\n",
    "        tmp[effect_size_col] = tmp[effect_size_col] / np.sqrt(tmp['n_ranks'])\n",
    "\n",
    "        # collapse ranks to their mean\n",
    "        tmp['avg_rank'] = tmp[rank_col].apply(lambda x: float(np.mean(x)) if len(x) else np.nan)\n",
    "        # if any NaN avg ranks, push them to the bottom\n",
    "        tmp['avg_rank'] = tmp['avg_rank'].fillna(tmp['avg_rank'].max() + 1 if not tmp['avg_rank'].isna().all() else 1e9)\n",
    "\n",
    "        # rerank (1..k) by ascending avg_rank\n",
    "        tmp = tmp.sort_values('avg_rank', ascending=True).reset_index(drop=True)\n",
    "        tmp['ranks'] = np.arange(1, len(tmp)+1)  # overwrite ranks with clean sequence\n",
    "        tmp = tmp.drop(columns=['n_ranks', 'avg_rank'])\n",
    "\n",
    "        # final trim by chosen_N using effect size (desc)\n",
    "        tmp = tmp.nlargest(min(chosen_N, len(tmp)), effect_size_col)\n",
    "\n",
    "        out[fid] = tmp.reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# --------- RUN THE PIPELINE ---------\n",
    "\n",
    "# A) find duplicate study groups\n",
    "dup_groups, raw_files = find_duplicate_files(ANNOTATED_DIR)\n",
    "\n",
    "# B) build per-file z-scored data\n",
    "dataset = {}\n",
    "csvs = [f for f in os.listdir(ANNOTATED_DIR) if f.endswith(\".csv\")]\n",
    "for fn in csvs:\n",
    "    fid = extract_file_id(fn)\n",
    "    if not fid: \n",
    "        continue\n",
    "    path = os.path.join(ANNOTATED_DIR, fn)\n",
    "    try:\n",
    "        dfz = convert_to_z_scores(path, multiply_by_effect_allele_count=True)\n",
    "        dataset[fid] = dfz\n",
    "    except Exception as e:\n",
    "        print(f\"  ! {fn}: {e}\")\n",
    "\n",
    "print(f\"[Step 2] Built z-scores for {len(dataset)} studies.\")\n",
    "\n",
    "# C) merge duplicates\n",
    "merged_data_dict = merge_duplicate_dataframes(dataset, dup_groups)\n",
    "print(f\"[Step 3] After merging, {len(merged_data_dict)} study entries remain.\")\n",
    "# quick sanity\n",
    "some_key = next(iter(merged_data_dict)) if merged_data_dict else None\n",
    "if some_key:\n",
    "    print(f\"  example {some_key}: {merged_data_dict[some_key].shape}, cols={list(merged_data_dict[some_key].columns)}\")\n",
    "\n",
    "# D) choose N via overlap sweep\n",
    "study_frames = list(merged_data_dict.values())\n",
    "chosen_N, chosen_score, max_score = maximize_overlap_fraction_threshold(\n",
    "    studies=study_frames,\n",
    "    id_column='hm_rsID',\n",
    "    weight_column='effect_weight',\n",
    "    max_N=MAX_N_SWEEP,\n",
    "    fraction=FRACTION\n",
    ")\n",
    "print(f\"[Step 4] chosen_N={chosen_N}  (score={chosen_score}, max={max_score})\")\n",
    "\n",
    "# E) standardize and rerank\n",
    "processed_data = standardize_effect_weights(merged_data_dict, chosen_N=chosen_N)\n",
    "print(f\"[Step 5] Standardized {len(processed_data)} studies.\")\n",
    "\n",
    "# F) save\n",
    "with open(OUT_PKL, 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "print(f\"[DONE] Wrote: {OUT_PKL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c06d9d-1fdd-49c0-9f04-8f8d3d637952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        rootX = self.find(x)\n",
    "        rootY = self.find(y)\n",
    "        if rootX != rootY:\n",
    "            self.parent[rootY] = rootX\n",
    "def extract_file_id(filename):\n",
    "    # Define a helper to extract and validate the file ID from a filename\n",
    "    match = re.match(r\"(PGS\\d+)_\", filename)\n",
    "    return match.group(1) if match else None\n",
    "def find_duplicate_files(folder_path):\n",
    "    file_data = {}\n",
    "    file_sets = {}\n",
    "    uf = UnionFind()\n",
    "    \n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_id = extract_file_id(filename)\n",
    "            if file_id:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Initialize union-find\n",
    "                uf.parent[file_id] = file_id\n",
    "                \n",
    "                # Check for necessary columns\n",
    "                if 'SNP_coord' in df.columns and 'hm_rsID' in df.columns:\n",
    "                    # Create sets of 'SNP_coord' and 'hm_rsID'\n",
    "                    snp_set = frozenset(df['SNP_coord'])\n",
    "                    rsid_set = frozenset(df['hm_rsID'])\n",
    "                    \n",
    "                    # Store in dictionaries for comparison\n",
    "                    file_data[file_id] = df\n",
    "                    file_sets[file_id] = {'SNP_coord': snp_set, 'hm_rsID': rsid_set}\n",
    "\n",
    "    # Identify matching files\n",
    "    file_ids = list(file_sets.keys())\n",
    "    \n",
    "    for i in range(len(file_ids)):\n",
    "        for j in range(i + 1, len(file_ids)):\n",
    "            id1, id2 = file_ids[i], file_ids[j]\n",
    "            if file_sets[id1]['SNP_coord'] == file_sets[id2]['SNP_coord'] or file_sets[id1]['hm_rsID'] == file_sets[id2]['hm_rsID']:\n",
    "                uf.union(id1, id2)\n",
    "\n",
    "    # Consolidate matches into groups, ensuring no self-only groups unless necessary\n",
    "    groups = {}\n",
    "    for file_id in file_ids:\n",
    "        root = uf.find(file_id)\n",
    "        groups.setdefault(root, []).append(file_id)\n",
    "    \n",
    "    # Filter out groups where each ID is only self-referencing, unless no other IDs are in the group\n",
    "    final_groups = {k: v for k, v in groups.items() if len(v) > 1 or len(set(v)) > 1}\n",
    "\n",
    "    return final_groups, file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7787cfe4-72b3-48eb-8ba0-91e1e5ddc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,List, Dict\n",
    "\n",
    "def convert_to_z_scores(\n",
    "    file_path: str,\n",
    "    effect_size_col: str = 'effect_weight',\n",
    "    rank_col: str = 'ranks',\n",
    "    effect_allele_col: str = 'effect_allele',\n",
    "    multiply_by_effect_allele_count: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, optionally multiplies the absolute value of effect_weight by the number of effect alleles\n",
    "    computed from effect_allele, converts the effect weights to Z-scores, and retains hm_rsID,\n",
    "    effect_weight (Z-score), and ranks.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - effect_size_col: Name of the column containing effect weights.\n",
    "    - rank_col: Name of the column containing ranks.\n",
    "    - effect_allele_col: Name of the column containing effect alleles.\n",
    "    - multiply_by_effect_allele_count: If True, multiplies the absolute value of effect_weight by the number of effect alleles\n",
    "      before calculating Z-scores.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns: hm_rsID, effect_weight (Z-score), ranks.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['hm_rsID', effect_size_col, rank_col, effect_allele_col]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the file '{file_path}'.\")\n",
    "\n",
    "    # Optionally multiply the absolute value of effect_weight by the number of effect alleles\n",
    "    if multiply_by_effect_allele_count:\n",
    "        # Define a function to compute effect_allele_count\n",
    "        def compute_effect_allele_count(effect_allele):\n",
    "            if pd.isnull(effect_allele):\n",
    "                return 0\n",
    "            else:\n",
    "                # Remove non-letter characters and count letters\n",
    "                letters = re.findall(r'[A-Za-z]', effect_allele)\n",
    "                return len(letters)\n",
    "        \n",
    "        # Compute effect_allele_count for each row\n",
    "        df['effect_allele_count'] = df[effect_allele_col].apply(compute_effect_allele_count)\n",
    "        # Multiply the absolute value of effect_weight by effect_allele_count\n",
    "        df[effect_size_col] = np.abs(df[effect_size_col]) * df['effect_allele_count']\n",
    "    else:\n",
    "        # Convert effect_weight to its absolute value\n",
    "        df[effect_size_col] = np.abs(df[effect_size_col])\n",
    "\n",
    "    # Calculate mean and standard deviation of effect weights\n",
    "    mu = df[effect_size_col].mean()\n",
    "    sigma = df[effect_size_col].std()\n",
    "\n",
    "    # Handle the case where standard deviation is zero\n",
    "    if sigma == 0 or pd.isnull(sigma):\n",
    "        df['effect_weight_z'] = 0\n",
    "    else:\n",
    "        # Compute Z-scores using the absolute values\n",
    "        df['effect_weight_z'] = (df[effect_size_col] - mu) / sigma\n",
    "\n",
    "    # Select the required columns\n",
    "    df_z = df[['hm_rsID', 'effect_weight_z', rank_col]].copy()\n",
    "    df_z.rename(columns={'effect_weight_z': 'effect_weight'}, inplace=True)\n",
    "\n",
    "    return df_z\n",
    "\n",
    "\n",
    "def merge_duplicate_dataframes(data_dict, duplicate_file_groups, effect_size_col='effect_weight', rank_col='ranks'):\n",
    "    \"\"\"\n",
    "    Merges DataFrames in data_dict that correspond to duplicate files, summing effect_weight and collecting ranks into a list.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: Dictionary with file_id as keys and DataFrames as values.\n",
    "    - duplicate_file_groups: Dictionary where each key is a group representative file_id,\n",
    "                             and the value is a list of duplicate file_ids (including the key).\n",
    "    - effect_size_col: Name of the effect weight column in the DataFrames.\n",
    "    - rank_col: Name of the rank column in the DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - A new data_dict with merged DataFrames for duplicate files.\n",
    "    \"\"\"\n",
    "    # Create a copy of data_dict to avoid modifying the original\n",
    "    merged_data_dict = data_dict.copy()\n",
    "    \n",
    "    # Keep track of file_ids that have been merged and should be removed\n",
    "    file_ids_to_remove = set()\n",
    "    \n",
    "    for group_id, file_ids in duplicate_file_groups.items():\n",
    "        # List to store DataFrames to be merged\n",
    "        dfs_to_merge = []\n",
    "        \n",
    "        for file_id in file_ids:\n",
    "            if file_id in merged_data_dict:\n",
    "                dfs_to_merge.append(merged_data_dict[file_id])\n",
    "                # Mark the file_id for removal if it's not the group_id\n",
    "                if file_id != group_id:\n",
    "                    file_ids_to_remove.add(file_id)\n",
    "            else:\n",
    "                print(f\"Warning: file_id '{file_id}' not found in data_dict.\")\n",
    "        \n",
    "        if dfs_to_merge:\n",
    "            # Concatenate the DataFrames\n",
    "            combined_df = pd.concat(dfs_to_merge, ignore_index=True)\n",
    "            # Group by 'hm_rsID' and aggregate\n",
    "            merged_df = combined_df.groupby('hm_rsID').agg({\n",
    "                effect_size_col: 'sum',\n",
    "                rank_col: lambda x: list(x)\n",
    "            }).reset_index()\n",
    "            # Update the data_dict with the merged DataFrame\n",
    "            merged_data_dict[group_id] = merged_df\n",
    "        else:\n",
    "            print(f\"No DataFrames found for group '{group_id}'.\")\n",
    "    \n",
    "    # Remove the merged file_ids from data_dict\n",
    "    for file_id in file_ids_to_remove:\n",
    "        del merged_data_dict[file_id]\n",
    "    \n",
    "    return merged_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e30c9e2-3711-4876-a86f-66ed0583e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_N(df, N=200, weight_column='effect_weight'):\n",
    "    if len(df) > N:\n",
    "        # Sort by 'effect_weight' in descending order and keep the top N\n",
    "        return df.nlargest(N, weight_column)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5ff5a5-623e-4162-a643-6953516df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "def filter_top_N_advanced(df, N=200, weight_column='effect_weight'):\n",
    "    \"\"\"\n",
    "    Advanced Top-N Filtering:\n",
    "    1. If df has more than N rows, determine a quantile cutoff for top fraction (N/len(df)).\n",
    "    2. Filter rows above this threshold.\n",
    "    3. If still more than N remain, select the top N.\n",
    "    Otherwise, return df as-is if ≤ N.\n",
    "    \"\"\"\n",
    "    if len(df) > N:\n",
    "        quantile_cutoff = 1 - (N / len(df))\n",
    "        threshold = df[weight_column].quantile(quantile_cutoff)\n",
    "        filtered = df[df[weight_column] >= threshold]\n",
    "\n",
    "        if len(filtered) > N:\n",
    "            return filtered.nlargest(N, weight_column)\n",
    "        else:\n",
    "            return filtered\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def overlap_metric(variant_sets):\n",
    "    \"\"\"\n",
    "    Compute a partial overlap metric based on how many studies each variant appears in.\n",
    "    Uses sum over variants of C(count, 2).\n",
    "    \"\"\"\n",
    "    all_variants = [v for s in variant_sets for v in s]\n",
    "    counts = Counter(all_variants)\n",
    "    score = sum((c*(c-1))//2 for c in counts.values())\n",
    "    return score\n",
    "\n",
    "def maximize_overlap_fraction_threshold(study_dataframes, id_column='hm_rsID', \n",
    "                                        weight_column='effect_weight', max_N=500, fraction=0.9):\n",
    "    \"\"\"\n",
    "    Choose N as the smallest N that achieves at least 'fraction' of the maximum overlap score.\n",
    "\n",
    "    Parameters:\n",
    "    - study_dataframes: list of pd.DataFrame, one per study.\n",
    "    - id_column: name of the variant ID column.\n",
    "    - weight_column: name of the effect weight column.\n",
    "    - max_N: maximum N to evaluate.\n",
    "    - fraction: the fraction of the maximum overlap score we want to achieve.\n",
    "\n",
    "    Returns:\n",
    "    - chosen_N: the smallest N that achieves at least 'fraction' * max overlap.\n",
    "    - chosen_score: the overlap score at chosen_N.\n",
    "    - max_score: the maximum overlap score achieved at any N.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for N in range(1, max_N + 1):\n",
    "        filtered_dataframes = [filter_top_N_advanced(df, N=N, weight_column=weight_column) for df in study_dataframes]\n",
    "        variant_sets = [set(df[id_column]) for df in filtered_dataframes if not df.empty and id_column in df.columns]\n",
    "        \n",
    "        if variant_sets:\n",
    "            current_score = overlap_metric(variant_sets)\n",
    "        else:\n",
    "            current_score = 0\n",
    "        scores.append((N, current_score))\n",
    "    \n",
    "    # Find the maximum score\n",
    "    max_score = max(score for _, score in scores)\n",
    "    \n",
    "    # Compute the threshold score\n",
    "    threshold_score = fraction * max_score\n",
    "    \n",
    "    # Find the smallest N that meets or exceeds the threshold score\n",
    "    # If all scores are zero, it will pick N=1.\n",
    "    chosen_N, chosen_score = min((N_s for N_s in scores if N_s[1] >= threshold_score),\n",
    "                                 key=lambda x: x[0],\n",
    "                                 default=(1, 0))  # default if none meets threshold\n",
    "    \n",
    "    return chosen_N, chosen_score, max_score\n",
    "\n",
    "# Example usage:\n",
    "# chosen_N, chosen_score, max_score = maximize_overlap_fraction_threshold(study_dataframes_list, fraction=0.9)\n",
    "# print(f\"Chosen N: {chosen_N}, Overlap Score at N: {chosen_score}, Max Score: {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153de83c-97a9-4075-b791-dd13476e337c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      3\u001b[39m study_frames = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_id, df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_data_dict\u001b[49m.items():\n\u001b[32m      5\u001b[39m     df_copy = df.copy(deep=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     study_frames.append(df_copy)\n",
      "\u001b[31mNameError\u001b[39m: name 'merged_data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from collections import Counter\n",
    "study_frames = []\n",
    "for file_id, df in merged_data_dict.items():\n",
    "    df_copy = df.copy(deep=True)\n",
    "    study_frames.append(df_copy)\n",
    "chosen_N, chosen_score, max_score = maximize_overlap_fraction_threshold(study_dataframes=study_frames, id_column='hm_rsID', \n",
    "                                        weight_column='effect_weight', max_N=500, fraction=0.9)\n",
    "\n",
    "print(chosen_N, chosen_score, max_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941079eb-07e4-4f0f-a524-2dee9b035588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_effect_weights(merged_data_dict, effect_size_col='effect_weight', rank_col='ranks'):\n",
    "    \"\"\"\n",
    "    Standardizes the summed effect_weight for each hm_rsID in the merged_data_dict and replaces the list of ranks \n",
    "    with their average. Then, reranks the hm_rsIDs based on these averages.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_data_dict: Dictionary where each key is a file_id and the value is a merged DataFrame.\n",
    "    - effect_size_col: Name of the effect weight column (default is 'effect_weight').\n",
    "    - rank_col: Name of the rank column containing lists of ranks (default is 'ranks').\n",
    "\n",
    "    Returns:\n",
    "    - A new dictionary with standardized effect weights and updated ranks for each DataFrame.\n",
    "    \"\"\"\n",
    "    standardized_data_dict = {}\n",
    "\n",
    "    for file_id, df in merged_data_dict.items():\n",
    "        df_copy = df.copy(deep=True)\n",
    "        df_copy = filter_top_N(df_copy, N=chosen_N)\n",
    "        # Calculate the average rank where ranks are in a list and update the ranks column\n",
    "        df_copy[rank_col] = df_copy[rank_col].apply(lambda x: np.mean(x) if isinstance(x, list) else x)\n",
    "\n",
    "        # Calculate the standard deviation based on the count of elements used to compute each average rank\n",
    "        df_copy['std_dev'] = df_copy[rank_col].apply(lambda x: np.sqrt(len(x)) if isinstance(x, list) else 1)\n",
    "\n",
    "        # Standardize the effect_weight\n",
    "        df_copy[effect_size_col] = df_copy.apply(\n",
    "            lambda row: row[effect_size_col] / row['std_dev'] if row['std_dev'] > 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Drop the intermediate column\n",
    "        df_copy.drop(columns=['std_dev'], inplace=True)\n",
    "\n",
    "        # Rerank based on the updated single rank values\n",
    "        df_copy.sort_values(by=rank_col, ascending=True, inplace=True)\n",
    "        df_copy[rank_col] = range(1, len(df_copy) + 1)\n",
    "\n",
    "        # Update the DataFrame in the new dictionary\n",
    "        standardized_data_dict[file_id] = df_copy\n",
    "\n",
    "    return standardized_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fa31502-2bcd-4f5e-a1c9-bac426d6a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate File Matches: {'PGS000026': ['PGS000026', 'PGS000876'], 'PGS000811': ['PGS000811', 'PGS000898', 'PGS001775'], 'PGS002280': ['PGS002280', 'PGS004600']}\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "folder_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\processed'\n",
    "matches, file_data = find_duplicate_files(folder_path)\n",
    "print(\"Duplicate File Matches:\", matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2adcdc6d-7856-40ff-8426-28811c49bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hm_rsID  effect_weight  ranks\n",
      "0     rs3752246       1.652779    3.0\n",
      "1    rs17265593       1.359162    4.5\n",
      "2    rs74615166       1.359162    4.5\n",
      "3      rs526904       0.918737    7.0\n",
      "4     rs6733839       0.184695   12.0\n",
      "5   rs115675626      -0.402539   15.0\n",
      "6     rs6572869      -0.402539   15.0\n",
      "7     rs4266886      -0.696156   21.5\n",
      "8    rs12679874      -0.696156   21.5\n",
      "9   rs117481827      -0.696156   21.5\n",
      "10   rs61822977      -0.842965   26.0\n",
      "11    rs1109581      -0.989773   29.0\n",
      "12    rs7920721      -0.989773   29.0\n",
      "13   rs10202748      -1.136582   31.0\n",
      "14    rs3740688      -0.989773   29.0\n",
      "15    rs7116190      -0.842965   26.0\n",
      "16    rs7145100      -0.842965   26.0\n",
      "17    rs2741342      -0.696156   21.5\n",
      "18    rs7831810      -0.696156   21.5\n",
      "19    rs2526378      -0.696156   21.5\n",
      "20   rs12590273      -0.549348   17.5\n",
      "21    rs7274581      -0.549348   17.5\n",
      "22    rs1476679      -0.402539   15.0\n",
      "23   rs78571833       0.037886   13.0\n",
      "24    rs9331888       0.331503   11.0\n",
      "25  rs115124923       0.478311   10.0\n",
      "26   rs11218343       0.625120    8.5\n",
      "27    rs7408475       0.625120    8.5\n",
      "28    rs1532277       1.065545    6.0\n",
      "29    rs2597283       2.093204    2.0\n",
      "30     rs543293       2.386821    1.0\n",
      "-1.2434497875801753e-14\n",
      "        hm_rsID  effect_weight  ranks\n",
      "0    rs74615166       3.290424    1.0\n",
      "1     rs3752246       0.317016    7.0\n",
      "2     rs1532277       0.251452    8.0\n",
      "3      rs543293       0.111464   10.0\n",
      "4      rs526904       0.001601   11.0\n",
      "5     rs7145100      -0.085227   12.0\n",
      "6   rs117481827      -0.124211   13.0\n",
      "7   rs115124923      -0.276603   16.5\n",
      "8     rs7408475      -0.276603   16.5\n",
      "9     rs7116190      -0.285463   18.0\n",
      "10    rs2741342      -0.455574   19.0\n",
      "11   rs61822977      -0.547718   20.0\n",
      "12    rs9331888      -0.572526   21.0\n",
      "13    rs7831810      -0.629230   22.0\n",
      "14   rs12679874      -0.675302   24.0\n",
      "15    rs7920721      -0.831237   27.0\n",
      "16   rs17265593      -0.902117   29.0\n",
      "17   rs10202748      -0.932241   30.0\n",
      "18    rs1109581      -0.935785   31.0\n",
      "19    rs2597283      -0.886169   28.0\n",
      "20    rs1476679      -0.739093   26.0\n",
      "21    rs3740688      -0.691250   25.0\n",
      "22    rs2526378      -0.664670   23.0\n",
      "23    rs6572869      -0.218127   15.0\n",
      "24  rs115675626      -0.157879   14.0\n",
      "25   rs12590273       0.180572    9.0\n",
      "26    rs7274581       0.651923    6.0\n",
      "27    rs4266886       0.692679    5.0\n",
      "28    rs6733839       1.201241    4.0\n",
      "29   rs78571833       1.690312    3.0\n",
      "30   rs11218343       2.498342    2.0\n",
      "-1.1102230246251565e-14\n"
     ]
    }
   ],
   "source": [
    "# Example usage for a single file\n",
    "file_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\Annotated\\PGS000026_annotated_dataset.csv'\n",
    "df_z = convert_to_z_scores(file_path)\n",
    "\n",
    "# View the result\n",
    "print(df_z)\n",
    "print(df_z['effect_weight'].sum())\n",
    "file_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\Annotated\\PGS000876_annotated_dataset.csv'\n",
    "df_z = convert_to_z_scores(file_path)\n",
    "\n",
    "# View the result\n",
    "print(df_z)\n",
    "print(df_z['effect_weight'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57ec73eb-ce42-4ae5-95c9-1c950e512e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_id = extract_file_id(filename)\n",
    "        if file_id:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df_z = convert_to_z_scores(file_path, multiply_by_effect_allele_count=True)\n",
    "            dataset[file_id] = df_z\n",
    "merged_data_dict = merge_duplicate_dataframes(dataset, matches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4697edb-d680-4e5b-a2b1-cc9db1788100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_data_dict.keys())\n",
    "# print(len(merged_data_dict.keys()))\n",
    "# print(merged_data_dict['PGS000026'])\n",
    "# print(merged_data_dict['PGS000026']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000811'])\n",
    "# print(merged_data_dict['PGS000811']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000025'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2d74f7e-89a0-49ff-a8c7-c33d75fc9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = standardize_effect_weights(merged_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f92681c7-0e7e-463a-b6c7-0a00eac7affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\data_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ead2490-ca12-40c3-9d42-f89a7e824d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_data_dict['PGS000026'])\n",
    "# print(merged_data_dict['PGS000026']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000811'])\n",
    "# print(merged_data_dict['PGS000811']['effect_weight'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277dafb8-946e-4842-b18c-260311af5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hm_rsID  effect_weight         ranks\n",
      "0    rs10202748      -2.068823  [31.0, 30.0]\n",
      "1     rs1109581      -1.925558  [29.0, 31.0]\n",
      "2    rs11218343       3.123462    [8.5, 2.0]\n",
      "3   rs115124923       0.201709  [10.0, 16.5]\n",
      "4   rs115675626      -0.560418  [15.0, 14.0]\n",
      "5   rs117481827      -0.820367  [21.5, 13.0]\n",
      "6    rs12590273      -0.368776   [17.5, 9.0]\n",
      "7    rs12679874      -1.371458  [21.5, 24.0]\n",
      "8     rs1476679      -1.141633  [15.0, 26.0]\n",
      "9     rs1532277       1.316997    [6.0, 8.0]\n",
      "10   rs17265593       0.457045   [4.5, 29.0]\n",
      "11    rs2526378      -1.360826  [21.5, 23.0]\n",
      "12    rs2597283       1.207035   [2.0, 28.0]\n",
      "13    rs2741342      -1.151730  [21.5, 19.0]\n",
      "14    rs3740688      -1.681023  [29.0, 25.0]\n",
      "15    rs3752246       1.969795    [3.0, 7.0]\n",
      "16    rs4266886      -0.003478   [21.5, 5.0]\n",
      "17     rs526904       0.920337   [7.0, 11.0]\n",
      "18     rs543293       2.498286   [1.0, 10.0]\n",
      "19   rs61822977      -1.390683  [26.0, 20.0]\n",
      "20    rs6572869      -0.620666  [15.0, 15.0]\n",
      "21    rs6733839       1.385936   [12.0, 4.0]\n",
      "22    rs7116190      -1.128427  [26.0, 18.0]\n",
      "23    rs7145100      -0.928192  [26.0, 12.0]\n",
      "24    rs7274581       0.102575   [17.5, 6.0]\n",
      "25    rs7408475       0.348517   [8.5, 16.5]\n",
      "26   rs74615166       4.649586    [4.5, 1.0]\n",
      "27    rs7831810      -1.325386  [21.5, 22.0]\n",
      "28   rs78571833       1.728198   [13.0, 3.0]\n",
      "29    rs7920721      -1.821010  [29.0, 27.0]\n",
      "30    rs9331888      -0.241023  [11.0, 21.0]\n",
      "        hm_rsID  effect_weight  ranks\n",
      "26   rs74615166       4.649586      1\n",
      "15    rs3752246       1.969795      2\n",
      "2    rs11218343       3.123462      3\n",
      "18     rs543293       2.498286      4\n",
      "9     rs1532277       1.316997      5\n",
      "28   rs78571833       1.728198      6\n",
      "21    rs6733839       1.385936      7\n",
      "17     rs526904       0.920337      8\n",
      "24    rs7274581       0.102575      9\n",
      "25    rs7408475       0.348517     10\n",
      "3   rs115124923       0.201709     11\n",
      "6    rs12590273      -0.368776     12\n",
      "16    rs4266886      -0.003478     13\n",
      "4   rs115675626      -0.560418     14\n",
      "20    rs6572869      -0.620666     15\n",
      "12    rs2597283       1.207035     16\n",
      "30    rs9331888      -0.241023     17\n",
      "10   rs17265593       0.457045     18\n",
      "5   rs117481827      -0.820367     19\n",
      "23    rs7145100      -0.928192     20\n",
      "13    rs2741342      -1.151730     21\n",
      "8     rs1476679      -1.141633     22\n",
      "27    rs7831810      -1.325386     23\n",
      "22    rs7116190      -1.128427     24\n",
      "11    rs2526378      -1.360826     25\n",
      "7    rs12679874      -1.371458     26\n",
      "19   rs61822977      -1.390683     27\n",
      "14    rs3740688      -1.681023     28\n",
      "29    rs7920721      -1.821010     29\n",
      "1     rs1109581      -1.925558     30\n",
      "0    rs10202748      -2.068823     31\n"
     ]
    }
   ],
   "source": [
    "print(merged_data_dict['PGS000026'])\n",
    "print(processed_data['PGS000026'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754a5fb-9019-47d8-97be-54cc03caf83d",
   "metadata": {},
   "source": [
    "### We have the processed data \n",
    "1. **processed_data** saved as \"data_dict.pkl\"\n",
    "2. **annotation_map** saved as \"annotation_map.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "238393c4-5765-45fa-9e24-5917471f0a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             SNP_coord      hm_rsID hm_chr       hm_pos effect_allele\n",
      "0          4_12348664   rs73225917      4   12348664.0             A\n",
      "1        11_119542229          NaN     11  119542229.0             T\n",
      "2          4_46680688  rs138116934      4   46680688.0             T\n",
      "3         17_65392050          NaN     17   65392050.0             C\n",
      "4          1_17117119  rs116308950      1   17117119.0             A\n",
      "...               ...          ...    ...          ...           ...\n",
      "2628033    6_32714451    rs2894381      6   32714451.0             A\n",
      "2628034   13_24940502    rs2104933     13   24940502.0             G\n",
      "2628035    6_32789520    rs9276711      6   32789520.0             T\n",
      "2628036    6_29970056    rs2256543      6   29970056.0             T\n",
      "2628037    6_33314404    rs3130099      6   33314404.0             G\n",
      "\n",
      "[2628038 rows x 5 columns]>\n",
      "dict_keys(['PGS000397', 'PGS000389', 'PGS003391', 'PGS004325', 'PGS004512', 'PGS000396', 'PGS000388', 'PGS004164', 'PGS000391', 'PGS002808', 'PGS000156', 'PGS004246', 'PGS004691', 'PGS000789', 'PGS004955', 'PGS000740', 'PGS000392', 'PGS000395', 'PGS004860', 'PGS000393', 'PGS005163', 'PGS002270', 'PGS004165', 'PGS003392', 'PGS000880', 'PGS004884'])\n"
     ]
    }
   ],
   "source": [
    "# check if we can import those data\n",
    "# Load annotation map\n",
    "annotation_map_check = pd.read_csv(r'/Users/martinli/Desktop/CancerGeneBot/data/annotation_map.csv', low_memory=False)\n",
    "# Load the dictionary back from the pickle file\n",
    "with open(r'/Users/martinli/Desktop/CancerGeneBot/Notebook/CancerPGS/data_dict.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "print(annotation_map_check.head)\n",
    "print(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1beb7dad-f32e-4585-b531-d55aadc8bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         hm_rsID  effect_weight  ranks\n",
      "0            nan      10.448514      1\n",
      "1    rs115993808       7.572244      2\n",
      "2            nan       7.528943      3\n",
      "3            nan       7.504337      4\n",
      "4            nan       7.484378      5\n",
      "..           ...            ...    ...\n",
      "282  rs113991523       4.037716    283\n",
      "283  rs149550370       4.036821    284\n",
      "284   rs79224290       4.036733    285\n",
      "285  rs115569840       4.034717    286\n",
      "286          nan       4.021968    287\n",
      "\n",
      "[287 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#PGS003992\n",
    "print(data_dict['PGS000397'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4da5be1-46d8-41e5-a3b1-b20b2f4ad08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "labels_df = pd.read_csv(os.path.join(r\"/Users/martinli/Desktop/CancerGeneBot/data\",\"LC_GWAS_Priority_Scores.csv\"))\n",
    "filtered_df = labels_df[labels_df['priority_score'].notna()]\n",
    "\n",
    "# Display the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(filtered_df['priority_score'], bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Priority Score')\n",
    "plt.xlabel('Priority Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig('y_dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2ba84-a64c-4047-b4e8-4d55e3a50f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
